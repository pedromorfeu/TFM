\documentclass[11pt,spanish,listoffigures,listoftables]{tfgetsinf}

\usepackage[utf8]{inputenc} 

\usepackage{listings}
\usepackage{color}

\graphicspath{ {images/} }

% Tex global defintions
\lstdefinestyle{default}{
    frame=tb,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=true,
    showspaces = false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    breaklines=true,
    breakatwhitespace=true,
    tabsize=2,
    showspaces = true
}

\definecolor{dkgreen}{rgb}{0,0.5,0}
\definecolor{dkblue}{rgb}{0,0,0.5}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstdefinestyle{python}{
    language=Python,
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{dkblue},
    commentstyle=\color{mauve},
    stringstyle=\color{dkgreen},
}
% End text global definitions


\title{Simulación de datos de sensores industriales}
\author{Pedro Henrique Mano Figueiredo Fernandes}
\tutor{Francisco Sánchez Cid}
\curs{2015-2016}

\keywords
{Machine learning, Big Data, PCA} 
{Machine learning, Big Data, PCA}
{Machine learning, Big Data, PCA} 

\begin{document}

% Abstract
\begin{abstract}[spanish]
Los entornos industriales hacen uso de sensores para obtener mediciones de varios factores en su cadena de producción. En las fases iniciales, la cantidad de datos generados es muy reducida, por lo que no es significativa para permitir la aplicación de técnicas de Big Data. Estas técnicas pueden descubrir muchos patrones en los datos, útiles para detección de anomalías o predicción futura. El proyecto descrito en este documento busca simular nuevos datos a través de un pequeña cantidad de datos generados por sensores industriales. La aplicación de estas técnicas se extiende a todo el contexto de proyectos IoT ({\em Internet of Things}), donde los sensores representan la principal fuente de datos. 
\end{abstract}

\begin{abstract}[english]
The industrial environments make use of sensors for acquiring metrics on several variables in their production pipeline. In the initial phases, the generated data has very low volume, making it hard to apply Big Data techniques. These techniques can bring much insight over the data, useful for anomaly detection or future prediction. The project described in this document seeks the simulation of new data using a small amount of data generated by industrial sensors. The application of such techniques can be extended to the entire context of IoT ({\em Internet of Things}), where sensors represent the main source of data.
\end{abstract}

\tableofcontents

\mainmatter

% Introducción
\chapter{Introducción}
Este proyecto tiene como fuente de datos uno o varios sensores de un mecanizado industrial de inyección de plástico. Los sensores efectúan mediciones de varios factores con una regularidad temporal, generando así muestras de datos en determinados intervalos de tiempo. Cuando se trata de sensores, es normal que los datos no tengan la calidad necesaria para aplicar técnicas matemáticas. Hay que tener en cuenta que la frecuencia de las mediciones no es necesariamente constante, que pueden existir interrupciones, ruido y redundancias. Además, en el ámbito de IoT ({\em Internet of Things}), se añade la inherente conectividad de los aparatos como factor de complejidad.

Las técnicas de Big Data, en particular {\em Machine Learning}, necesitan gran cantidad de datos para poder inducir modelos matemáticos que expliquen esos datos. Cuanto más datos mejor, para un aprendizaje más robusto y fiable. Sin embargo, al principio de los proyectos, la cantidad de datos recolectados suele ser pequeña y insuficiente para poder extraer conocimiento significativo de los mismos. El objetivo de este proyecto es aprender y simular el comportamiento de uno o varios sensores a través de una cantidad reducida de datos recolectados en los mismos. 

Una vez preparado el {\em dataset}, es necesario aprender un modelo matemático que lo describa. Las técnicas usadas para ese efecto son del ámbito de {\em Machine Learning}. 

%La fase inicial del análisis es la ingesta de los datos. El dataset proporcionado es de reducida dimensión, tiene tan solo 5MB, por lo que la tarea de ingestión no es intensiva. Sin embargo, hay que tratar los datos en bruto antes de empezar a usarlos. El fichero de datos es de texto, así que hay que hacer determinadas conversiones para poder usar tipos más específicos, como sean fechas y números. El primer problema tiene que ver con los formatos de fecha, que no son correctos para el locale España, lo que exige una adaptación de los algoritmos de parsing.
%Los pasos descritos son muy comunes en análisis de datos y en particular en el {\em pipeline} de Big Data.
    
    \section{Estructura del documento}
    
    Este documento se estructura de la siguiente forma: la descripción del problema; el estado del arte; un análisis sobre el {\em dataset} y respectivas transformaciones para adecuar los datos; las soluciones propuestas y respectiva base teórica; la experimentación, resultados y discusión; conclusiones del trabajo realizado y posibles mejoras; y termina con las fuentes bibliográficas que han servido de base del estudio.

% Descripción del problema
\chapter{Descripción del problema}
El paradigma de IoT ({\em Internet of Things}) tiene mucha presencia en la industria. Se utilizan sensores para controlar determinados factores en las cadenas de producción. Los datos generados son de gran importancia. Con el debido tratamiento y análisis, estos datos pueden ayudar descubrir conocimiento nuevo. En el entorno industrial, ese conocimiento se puede traducir en la anticipación de anomalías o mejoras de rendimiento de las máquinas, con todo el beneficio que eso conlleva.

La extracción de conocimiento de los datos se hace con técnicas estadísticas de {\em Machine Learning}. Estas técnicas están relacionadas con Big Data por la simple regla de que cuantos más datos mejor. Los modelos matemáticos que explican los datos son más robustos si hay mucho volumen de datos. Sin embargo, en las etapas iniciales de implantación de IoT no hay mucha cantidad de datos, lo que dificulta la aplicación de {\em Machine Learning}.

Hay que tener en cuenta que los datos de sensores suelen tener problemas de calidad, sobre todo relacionados con ruido, redundancias o frecuencia de muestro. Por ejemplo, si un sensor mide la temperatura de una máquina, esa medición sufre interferencias del ambiente, por lo que la medición no es completamente objetiva. Ese efecto se conoce por ruido. Cuanto a las redundancias, pueden suceder en casos de pérdida de conectividad, en que el sensor vuelve a dar una métrica aunque la haya dado ya anteriormente. Esto sería, también, un caso de cambio de la frecuencia de muestreo, 	que se traduce en irregularidades de la serie temporal asociada a los datos. 

El objetivo de este proyecto es aprender y simular datos de sensores industriales. Los nuevos datos, de mucho mayor volumen, podrán ser usados para probar y depurar los algoritmos de detección de averías y de predicción desarrollados para entornos Big Data. 

% Estado del Arte
\chapter{Estado del Arte}
Los problemas de ruido y redundancias en los datos son comunes y están bastante estudiados. Para solucionar ese problema, las técnicas de reducción de dimensionalidad, en que los datos son reducidos a lo esencial, suelen tener buenos resultados. PCA ({\em Principal Component Analysis} es la técnica más usual. PCA se usa para descomponer un conjunto de datos multivariante en un grupo de componentes ortogonales que mejor explican la varianza. El ajuste a los datos es de tipo lineal.

Las primeras componentes son las que mayor varianza explican y por eso se llaman principales. El orden de las componentes es relevante, la primera es la más explicativa, la segunda la que más explica quitando la primera, y así sucesivamente. Así que es de esperar que lo que están explicando las últimas sea, realmente, el ruido de la señal. El artículo \textit{A Tutorial on Principal Component Analysis} \cite{shlens} deja patente esa evidencia en el objetivo de filtrar el ruido y redundancias para obtener la dinámica importante de los datos.

Aunque PCA no tenga capacidades de predicción (pertenece a la categoría de {\em unsupervised learning}), se puede usar para simular nuevos datos, como se defiende en el artículo 'The Truth About Principal Component and Factor Analysis'. Aparte de las componentes que explican los datos, PCA obtiene también las proyecciones a lo largo de las componentes. Se pueden aprender las series de los {\em scores} de las proyecciones, reemplazar por nuevos valores y luego transformar de vuelta a un vector de características originales.

Las observaciones de los sensores tienen una implícita ordenación cronológica (y así lo tendrán también las componentes principales). El estudio de esa relación se denomina análisis de series temporales. Con este análisis ahí se puede llegar a explicar mejor la serie y hacer predicciones sobre el futuro de la misma. 

Los modelos {\em Exponential smoothing} y {\em AutoRegressive Integrated Moving Average} (ARIMA) son los más usados para predicción de series temporales. {\em Exponential smoothing} se basan en la descripción de tendencia y de variación estacional. Pero, en general, observaciones sucesivas suelen tener dependencia entre si, como se describe en el libro \textit{Introduction to Time Series Analysis and Forecasting} \cite{montgomery}. Para incorporar esta dependencia se usan los modelos ARIMA. Los modelos ARIMA son un flexible y poderoso método para análisis de series temporales y predicción. A lo largo de los años, se vienen usando con éxito en varios problemas de investigación y en la práctica.

% Solución propuesta
\chapter{Solución propuesta}
La solución propuesta para la simulación de datos empieza con la proyección de los datos del espacio multivariante original a un espacio ortogonal más reducido. Esta técnica de {\em Machine Learning} se conoce por PCA ({\em Principal Component Analysis}). El nuevo espacio (con menos dimensiones) contiene las componentes que mejor explican los datos, por lo que se eliminan ruido y redundancias. 

Para cada componente, se efectúa la generación de nuevos valores con distribución Gaussiana. Esta simulación puede ser de miles o millones de nuevos puntos - no es una tarea computacional exigente y además tendrá utilidad más adelante. La simulación Gaussiana servirá como base del estudio, una vez que, al ser invertida, genera una población que es idéntica a la original. La comprobación de dicha similitud de poblaciones se hace a través del estadístico T-cuadrado de Hotelling. Y esta será nuestra hipótesis nula.

Los datos originales tienen un determinado sentido temporal. Las componentes lo tendrán también, así que se procede al análisis de la serie temporal de esas componentes. El modelo propuesto es ARIMA ({\em Autoregressive Integrated Moving Average}). Los modelos de series temporales se ajustan a los datos para mejor comprensión de estos o para predicción de nuevos puntos en la serie. El objetivo es dotar la simulación de un patrón temporal aprendido previamente.

Teniendo en cuenta que el estudio se sustenta en la hipótesis nula de la simulación Gaussiana, para cada predicción de las varias componentes con marca temporal, se buscará el punto multi-dimensional más cercano en la simulación Gaussiana. El punto elegido se usará para re-alimentar el modelo ARIMA aprendido. Llegados aquí, tenemos nuevos valores simulados de las componentes y además con carácter temporal. Al re-proyectar este espacio al espacio original obtenemos nuevos valores (simulados) para las características originales.

La simulación Gaussiana debe tener gran volumen para que la simulación final sea efectiva. Cuantos más puntos tenga mejor, para que la búsqueda encuentre puntos más cercanos. Con un volumen de datos muy grande, los paradigmas de programación convencionales no tienen capacidad de respuesta. Se adoptan herramientas Big Data ({\em Spark} y {\em MongoDB}) para permitir escalar la parte de simulación Gaussiana, su almacenamiento y la búsqueda de puntos cercanos.


    \section{Reducción de dimensionalidad - PCA}
    PCA ({\em Principal Components Analysis}) pertenece a la familia de aprendizaje no supervisado ({\em unsupervised learning}), de {\em Machine Learning}, donde no existen etiquetas o clases a predecir. Los datos de entrenamiento son simplemente observaciones, sin estar clasificados. Las técnicas de {\em unsupervised learning} se usan para descubrir grupos de similitud (clusters) en los datos; o para determinar la distribución de los datos en el espacio original; o también, que interesa a este estudio, para proyectar los datos en espacios de menos dimensiones.

    Pocas dimensiones suelen ofrecer mejor {\em insight} sobre los datos. Como primera ventaja, permiten la aplicación de técnicas de visualización - resulta muy difícil visualizar datos en más de 3 dimensiones (incluso en 3 dimensiones puede ser difícil). Además, las componentes que no son principales (menos explicativas) suelen ser ruido o redundancia - con esta técnica se pueden atenuar los efectos de estos. La simulación de datos puede usar la ventaja de la eliminación de ruido y redundancia, generando así datos con mayor entidad.
    
    PCA descompone un {\em dataset} multivariante en un determinado número de componentes ortogonales que son los que más explican la varianza de ese {\em dataset}. El número de componentes es menor o igual al número de características originales. Por ejemplo, si dos variables son directamente proporcionales, basta con una sola componente para explicar el comportamiento de las dos - esa correlación quedará representada en la matriz calculada por PCA. Si se convierte esa componente al espacio original de dos características, se obtienen los valores originales.
    
    La dirección de la primera componente principal es aquella a lo largo de la cual las observaciones varían más.
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth]{GaussianScatterPCA.png}
        \caption{Datos con distribución Gaussiana y las dos primeras componentes PCA.}
        \label{fig:pca1}
    \end{figure}    
   
    Por ejemplo, la Figura~\ref{fig:pca1} representa un conjunto de datos de dos dimensiones con distribución Gaussiana. La flecha más grande representa la dirección de la primera componente principal de los datos. Se puede verificar a simple vista que esta es la dirección con mayor variabilidad en los datos. Si trazamos una línea imaginaria a lo largo de esa flecha (como la línea verde) y proyectamos en ella los datos, tendremos la proyección con mayor variabilidad. 
    
    La dirección de segunda componente está representada por la otra flecha. Al proyectar los datos en la línea imaginaria azul, podemos imaginar que el espectro de variabilidad es inferior al la primera.
   
    La proyección de un punto a una línea es muy sencilla: basta con buscar el punto más cercano en esa línea. Los puntos azules en la Figura~\ref{fig:pca_projection} representan la proyección de los puntos originales en la línea.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth]{ProjectionPCA.jpg}
        \caption{Proyección de puntos a una línea.}
        \label{fig:pca_projection}
    \end{figure}    
    
   En términos matemáticos, PCA es una transformación lineal ortogonal. Consideremos la matriz \(X\), constituida por \(n\) líneas de observaciones y \(m\) características. El objetivo es proyectar los datos a un espacio con dimensionalidad \(d < m\), de forma que se maximice la varianza de los datos proyectados. La transformación de PCA se define por la ecuación:
   \begin{equation}
   Y = X \cdot w
   \end{equation}
   
   Que se interpreta de la siguiente forma: una matriz \(m \times k\) de pesos ({\em loadings}) \(w\) transforma la matriz \(n \times m\) \(X\) en la matriz \(n \times k\) de componentes principales ({\em scores}) \(Y\).

   Para obtener la primera componente, el primer vector de {\em loadings} \(w\) debe maximizar la varianza. Siguiendo el razonamiento del libro \textit{Pattern Recognition and Machine Learning} \cite{bishop}, consideremos el vector \(w_{1}\), que, por conveniencia (y sin pérdida de generalización) debe ser un vector unitario de modo que \(w_{1}^{T}w_{1}=1\). Cada punto de X, \(x_{i}\), es proyectado para el escalar \(w_{1}^{T}x_{i}\). La media de los datos proyectados es \(w_{1}^{T} \overline{x}\), donde \(\overline{x}\) es la media dada por:
   \begin{equation}
   \overline{x} = \frac{1}{n}\sum_{i=1}^{n}x_{i}
   \end{equation}
   
   Y la varianza de los datos proyectados:
   \begin{equation}
   \frac{1}{n}\sum_{i=1}^{n}x_{i}\big\{w_{1}^{T}\cdot x_{i}-w_{1}^{T}\cdot \overline{x}\big\} = w_{1}^{T}\cdot S\cdot w_{1}
   \end{equation}
   
   Donde \(S\) es la matriz de varianzas-covarianzas definida por:
   \begin{equation}
   S=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\overline{x})\cdot (x_{i}-\overline{x})^{T}
   \end{equation}
   
   Ahora maximizamos la varianza de los datos proyectados \(w_{1}^{T}\cdot S\cdot w_{1}\) con respecto a \(w_{1}\). Hay que restringir la maximización para prevenir que tienda para infinito. La restricción viene de la normalización \(w_{1}^{T}w_{1}=1\). Para forzar la restricción introducimos un multiplicador de Lagrange \(\lambda_{1}\):
   \begin{equation}
   w_{1}^{T}\cdot S\cdot w_{1} + \lambda_{1} (1 - w_{1}^{T}\cdot w_{1})
   \end{equation}
   
   Que tiene un punto estacionario cuando:
   \begin{equation}
   S\cdot w_{1} = \lambda_{1} \cdot w_{1}
   \end{equation}
   
   Esto define que \(w_{1}\) es un vector propio de \(S\). Si multiplicamos las parte izquierda por \(w_{1}^{T}\), teniendo en cuenta que \(w_{1}^{T}w_{1}=1\), vemos que la varianza es dada por:
   \begin{equation}
   w_{1}^{T}\cdot S\cdot w_{1} = \lambda_{1} 
   \end{equation}
   
   Así que la varianza será máxima cuando se defina \(w_{1}\) igual al vector propio con el máximo valor propio \(\lambda_{1}\).
   
    \subsection{Singular Value Decomposition}
    PCA está muy relacionado con una técnica matemática llamada {\em Singular Value Decomposition} (SVD), tanto que muchas veces los nombres se usan intercambiados. De hecho, el algoritmo de PCA de {\tt scikit-learn} usa la descomposición SVD de {\tt numpy}. SVD es un método más general de entender el cambio de base.
    
    La representación de la descomposición en valores singulares es:
    \begin{equation}
    X = U \Sigma W^{T}
    \end{equation}
    
    Donde:
    \begin{itemize}
    \item \(U\) es una matriz \(n \times n\), en que las columnas son vectores unitarios ortogonales de tamaño \(n\).
    \item \(\Sigma\) es una matriz diagonal \(n \times m\) de números positivos \(\sigma_{i}\), llamados valores singulares de \(X\).
    \item \(W\) es una matriz \(m \times m\), cuyas columnas son vectores unitarios ortogonales de tamaño \(p\).
    \end{itemize}
    
    La ecuación 4.2 indica que una matriz \(X\) puede ser convertida en una matriz ortogonal, una matriz diagonal y otra matriz ortogonal. O, dicho de otra forma, corresponde a una rotación, un estiramiento y otra rotación.
    
    \subsection{El método NIPALS}
    SVD es el método general para cálculo de PCA. Pero si lo aplicamos a un {\em dataset} multidimensional muy grande puede tener problemas, porque calcula todas las componentes de una vez (calcula toda la matriz de varianzas-covarianzas de \(X\)). NIPALS significa {\em Non-linear Iterative Partial Least Squares} y es un algoritmo iterativo que solo calcula las primeras componentes principales.
    
    Para cada componente, el algoritmo NIPALS itera hasta que el {\em score} \(y_{1}\) tenga convergencia. Antes de la iteración, se atribuye un valor aleatorio al {\em score} \(y_{1}\). La convergencia se detecta cuando la variación del {\em score} \(y_{1}\) respecto a la iteración anterior es muy pequeña. 
    
     En cada iteración, para encontrar el {\em loading} \(w_{1}\) se proyecta \(X\) hacía \(y_{1}\) - regresión del {\em score} respecto a cada columna de \(X\) - y se guarda el coeficiente de regresión en el {\em loading}. Luego se proyecta \(X\) hacía \(w_{1}\) para encontrar el {\em score} \(y_{1}\) - regresión del {\em loading} respecto a cada línea de \(X\) - y se guarda el coeficiente de regresión en el {\em score}. 
    
    Una vez convergido, para calcular la siguiente componente se resta el producto \(y_{1} \cdot w_{1}\) a \(X\). Esto corresponde a restar a \(X\) la parte de los datos explicada por \(y_{1}\) y \(w_{1}\).
    
%    El nuevo espacio está constituido por variables independientes y ortogonales. Para cada componente, se calculan valores aleatorios con distribución normal. Estos son los scores de las componentes de los nuevos datos. Al proyectar la nueva matriz de componentes de vuelta al espacio original, se obtienen datos simulados para todas las características. Si la validación de esta técnica demuestra que es correcta, se puede usar para la simulación de nuevos datos. La validación de los datos generados pasa por la aplicación de estadísticos de comparación de las muestras originales y las nuevas. La técnica utilizada es el test de Hotelling T-squared, que prueba la similitud de las poblaciones. Los resultados demuestran que es viable usar esta técnica de simulación de datos.

    \section{Series temporales - ARIMA}
    Las series temporales surgen frecuentemente en la monitorización de procesos industriales. Cada observación de un sensor tiene una marca temporal asociada. El conjunto de las observaciones ordenadas de forma cronológica se denomina serie temporal. En este capítulo se demostrará el importante valor que tiene esa marca temporal en el análisis de las señales. 
    
    El análisis de series temporales asienta en la suposición de que los datos tomados en determinados puntos en el tiempo tienen una estructura interna (correlación, tendencia o variación estacional) que hay que tener en cuenta. El objetivo del análisis de series temporales es extrapolar datos futuros a través de los datos pasados. 
    
    ARIMA significa ({\em Autoregressive Integrated Moving Average}) y, junto con ({\em Exponential Smoothing}), es uno de los métodos más populares de análisis de series temporales. ARIMA incorpora formalmente la dependencia entre observaciones sucesivas en el modelo, mientras {\em Exponential Smoothing}) no lo hace de forma tan eficiente.
    
    La componente AR ({\em Autoregressive} - auto-regresiva) del modelo ARIMA se refiere al uso de observaciones anteriores en una ecuación regresiva para predecir valores futuros. A esta componente se asocia un parámetro \(p\), que corresponde al numero de puntos anteriores a incluir en el modelo. La componente MA ({\em Moving Average} - media móvil) representa el error del modelo como combinación de términos de error anteriores. El parámetro asociado es \(q\) y corresponde al numero de términos de error anteriores a usar. La componente I ({\em Integrated}) indica la diferenciación de los datos, que corresponde a calcular la diferencia entre los valores y sus antecesores. El parámetro \(d\) es el numero de diferenciaciones a aplicar en el modelo. De esta forma, los modelos ARIMA son generalmente definidos por \(ARIMA(p, d, q)\).
    
    Es importante resaltar que el modelo \(ARIMA(p, d, q)\) asume que la serie no tiene variación estacional y tendencia. Potencialmente es necesario eliminar esas características antes de crear el modelo. Esto nos lleva al concepto de {\em stationarity} (estacionaridad).
    
    \subsection{Estacionaridad}
    Las series temporales son diferentes de un problema de regresión normal porque son dependientes del tiempo. Así que el supuesto básico de la regresión lineal de que las observaciones son independientes no se verifica en este caso. Para poder aplicar el modelo ARIMA es necesario que la serie sea estacionaria. Una serie es estacionaria cuando la media y varianza, si existen, no cambian en función del tiempo.
    
    
    

    
    
    \subsection{Parámetros del modelo}
    TODO
    AR I MA
    p, q, d
    
    We can specify non-seasonal ARIMA structure and fit the model to de-seasonalize data. Parameters (1,1,1) suggested by the automated procedure are in line with our expectations based on the steps above; the model incorporates differencing of degree 1, and uses an autoregressive term of first lag and a moving average model of order 1
    
    
    \subsection{Predicción}
    TODO
    
   
    Stationarity and differencing: "This shows one way to make a time series stationary — compute the differences between consecutive observations. This is known as differencing.
    The estimator with the smallest MSE is the best
    If the data are equi-spaced, the time variable, or index, does not need to be explicitly given. The time variable may sometimes be explicitly used for plotting the series. However, it is not used in the time series model itself
    
    The difference between the observation y1 and the value obtained by fitting a timeseries model to the data, or a fitted value .Yr defined above, is called a residual
     the human eye can be a very sophisticated data analysis tool
    
    
        "Causal variables will typically include data such as GRPs and price and also may incorporate data from consumer surveys or {\bf exogenous} variables such as GDP"
    
    Steps:
    Plot, examine, and prepare series for modeling
    Extract the seasonality component from the time series
    Test for stationarity and apply appropriate transformations
    Choose the order of an ARIMA model
    Forecast the series
    

    
% Experimentación y resultados
\chapter{Experimentación y resultados}
TODO

    % Dataset
    \section{Dataset}
    El dataset proporcionado es de reducida dimensión, tiene tan solo 5MB, por lo que la tarea de ingestión no es intensiva. Sin embargo, hay que tratar los datos en bruto antes de empezar a usarlos. El fichero de datos es de texto, así que hay que hacer determinadas conversiones para poder usar tipos más específicos, como sean fechas y números. El primer problema tiene que ver con los formatos de fecha, que no son correctos para el {\em locale} España en {\em Windows}, lo que exige una adaptación de los algoritmos de {\em parsing}, como se describe en el apartado 'Transformación'.
    
        \subsection{Estructura}
        Un breve análisis del {\em dataset} en un editor de texto muestra que tiene un formato de campos separados por espacios y tabulaciones. La cantidad de espacios es variable:
    
        \lstset{style=default}
        \lstset{
          showspaces = true
        }
        \begin{lstlisting}[caption=Ejemplo del {\em dataset}.]
        Tiempoinicio                    	APHu                            	APVs ...
        
        06-oct-2015 21:57:03 	44.6             	69.3 ...
        06-oct-2015 21:57:12 	45.1             	69.0 ...
        06-oct-2015 21:57:21 	44.8             	69.8 ...
        ...
        \end{lstlisting}
        
        \lstset{
          showspaces = false
        }
        La primera línea contiene un {\em header} (cabecera), con 15 nombres: 
        \begin{lstlisting}[caption={\em Header} del {\em dataset}.]
        Tiempoinicio APHu APVs ACPv ZSx ZUs H7x H1x H2x H6x H3x H4x H5x ACPx Svo
        \end{lstlisting}
        
        Se puede verificar también que hay una línea vacía después del {\em header}. El primer campo tiene un formato de fecha/hora y los demás campos tienen formato decimal.
    
        \subsection{Transformación}
        Para la ingestión y transformación de los datos se han usado librerías muy útiles y con muchas funcionalidades que facilitan bastante esas tareas: en los scripts Python se ha usado el paquete {\em Pandas} y en R la función {\em read.csv2} del paquete {\em utils}.
        
        Las 14 variables decimales no ofrecen problemas en la ingestión del {\em dataset}. Para asegurar el formato decimal, es conveniente definir el separador decimal como punto ('.'). Con eso es suficiente para un {em parsing} correcto.
        
        Las fechas son más complejas de procesar. El formato de mes da indicios de estar escrito en castellano: {\tt oct, dic, mar, abr, may, jun}. Sin embargo, en {\em Windows}, el {\em locale} España usa un punto ('.') como {\em standard} en la abreviación de mes, por ejemplo 'oct.'. Así que el {\em parsing} de fechas tiene que ser ajustado si el sistema operativo es {\em Windows}. La estrategia ha sido el uso de una expresión regular para añadir el punto ('.') necesario en las abreviaciones de meses, como se puede ver en los siguientes {\em snippets} Python y R:
        
        \lstset{style=python}
        \begin{lstlisting}[caption={\em Parsing} de fechas en Python.]
        def parse_date(date_string):
            locale_date_string = re.sub("(.+-)(.+)(-.+)", "\\1\\2.\\3", date_string)
            return datetime.strptime(locale_date_string, "%d-%b-%Y %H:%M:%S")
        \end{lstlisting}
        
        \begin{lstlisting}[caption={\em Parsing} de fechas en R.]
        data$Tiempoinicio <- sub("(\\d+-)(\\w+)(-\\d+\\s\\d+:\\d+:\\d+)", "\\1\\2.\\3", data$Tiempoinicio)
        data$Tiempoinicio <- as.POSIXct(data$Tiempoinicio, format="%d-%b-%Y %H:%M:%S")
        \end{lstlisting}
        
        En este caso particular de las fechas, el problema no está en el {\em dataset realmente}. Los {\em standards} de fechas varian en cada sistema operativo. La idea de exponer este caso no es más que transmitir la necesidad de ajustar y normalizar los datos cuando provienen de sensores.
        
        El resultado final es un {\em dataset} estructurado con una marca temporal como índice y 14 características.
        
        \subsection{Distribución de los datos}
        
        Las mediciones de los sensores se distribuyen por varios días, no siempre consecutivos. Los días con mediciones son algunos 
        \begin{itemize}
        \item 6-9, 12 de octubre de 2015
        \item 14 de diciembre de 2015
        \item 7, 8, 11, 14 de marzo de 2016
        \item 6, 29 de abril de 2016
        \item 2, 26 de mayo de 2016
        \item 8, 17, 20 de junio de 2016
        \end{itemize}
        
        En cada día las muestras son dispares y en momentos distintos del día, probablemente por corresponder a distintas pruebas. Así, también las frecuencias de muestreo son variables, en algunos días parece indicar una frecuencia de 10 segundos pero hay otros donde son de 1 minuto.
    
    % PCA
    \section{PCA}
    TODO
    
        \subsection{PCA iterativo - NIPALS}
        Sin embargo, el cálculo de SVD es muy intensivo para matrices grandes, porque calcula la matriz de varianzas-covarianzas para todos los componentes. Esto implica un gran consumo de memoria y CPU. Como alternativa, se propone usar una técnica iterativa llamada NIPALS ({\em Nonlinear Iterative Partial Least Squares}), que usa el numero de componentes reducido.
    TODO: explain NIPALS algorithm        TODO
        
        \subsection{Test de hipótesis - Estadístico T-cuadrado de Hotelling}
        TODO
        \subsection{Simulación con distribución Gaussiana - hipótesis nula}
        TODO
    
    % ARIMA
    \section{ARIMA}
    TODO
        \subsection{Simulación con todo el dataset}
        TODO
        \subsection{Simulación con datos de 1 día}
        TODO
        \subsection{Simulación con búsqueda de puntos cercanos de la Gaussiana}
        TODO
    
    % Big Data
    \chapter{Big Data}
    TODO
        \section{MongoDB}
        TODO
        \section{Spark}
        TODO
 
% Conclusiones
\chapter{Conclusiones}
TODO

% Trabajos futuros
\chapter{Trabajos futuros}

\begin{thebibliography}{10}

\bibitem{shlens}
   Jon Shlens.
   \newblock \textit{A TUTORIAL ON PRINCIPAL COMPONENT ANALYSIS. Derivation, Discussion and Singular Value Decomposition.}
   \newblock Version 1, 25 March, 2003.

\bibitem{light}
   Unknown Authors.
   \newblock \textit{The Truth about Principal Components and Factor Analysis.}
   \newblock 28 September, 2009.
   
\bibitem{bishop}
   Christopher M. Bishop.
   \newblock \textit{Pattern Recognition and Machine Learning.}
   \newblock Springer, 2006.

\bibitem{montgomery}
   Montgomery, Douglas C.
   Jennings, Cheryl L.
   Kulahci, Murat
   \newblock \textit{Introduction to Time Series Analysis and Forecasting.}
   \newblock Springer, 2006.

\bibitem{WAR}
   \textit{Principal component analysis (PCA).}
   \newblock Consultar 
   \url{http://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca}.

\end{thebibliography}
\cleardoublepage

%\APPENDIX
%
%\chapter{Configuración del sistema}
%TODO
%    \section{Fase de inicialización}
%    TODO

\end{document}
